#!/usr/bin/env python3
"""
Scraper optimis√© pour asako.mg - Scraping direct vers MySQL
Objectif: R√©cup√©rer au moins 50 offres r√©elles pour le hackathon
"""

import urllib.request
import re
from datetime import datetime, timedelta
import time
import sys
import os

# Ajouter le chemin parent pour les imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from database.models import JobOffer, SessionLocal
    print("‚úÖ Modules MySQL charg√©s")
except ImportError as e:
    print(f"‚ùå Erreur import MySQL: {e}")
    print("‚ö†  Mode scraping seul activ√©")
    JobOffer = None
    SessionLocal = None

class AsakoScraper:
    def __init__(self, use_database=True):
        self.base_url = "https://www.asako.mg"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 SafeAI-Hackathon/1.0'
        }
        self.use_database = use_database and JobOffer is not None
        print(f"ü§ñ Scraper initialis√© (MySQL: {self.use_database})")
    
    def fetch_page(self, url):
        """R√©cup√©rer une page HTML avec retry"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                req = urllib.request.Request(url, headers=self.headers)
                with urllib.request.urlopen(req, timeout=20) as response:
                    if response.status == 200:
                        html_content = response.read().decode('utf-8', errors='ignore')
                        print(f"‚úÖ Page charg√©e: {url}")
                        return html_content
                    else:
                        print(f"‚ö†  Statut {response.status} pour {url}")
            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"‚è≥ Tentative {attempt + 1}/{max_retries} √©chou√©e pour {url}: {e}")
                    time.sleep(2)
                else:
                    print(f"‚ùå Erreur pour {url} apr√®s {max_retries} tentatives: {e}")
        return None
    
    def extract_offers_html(self, html):
        """Extraire le HTML de chaque offre - version am√©lior√©e"""
        if not html:
            return []
        
        # Pattern am√©lior√© pour d√©tecter les offres
        patterns = [
            r'<div class="d-flex item">(.*?)</div>\s*</div>\s*</div>\s*</div>',
            r'<div class="[^"]*item[^"]*">(.*?)</div>\s*</div>\s*</div>',
            r'<div[^>]*class="[^"]*offer[^"]*"[^>]*>(.*?)</div>\s*</div>'
        ]
        
        for pattern in patterns:
            offers_html = re.findall(pattern, html, re.DOTALL)
            if offers_html:
                print(f"üìä {len(offers_html)} offres d√©tect√©es avec pattern")
                return offers_html
        
        # Fallback: chercher par structure commune
        offers_sections = re.findall(r'<h3>\s*<a[^>]*>.*?</a>\s*</h3>.*?<span class="date-pub">.*?</span>', html, re.DOTALL)
        if offers_sections:
            print(f"üìä {len(offers_sections)} offres d√©tect√©es (fallback)")
            return offers_sections
        
        print("‚ö†  Aucune offre d√©tect√©e avec les patterns actuels")
        return []
    
    def parse_offer(self, html):
        """Parser une offre individuelle - version robuste"""
        try:
            # Titre - version plus robuste
            title = "Non sp√©cifi√©"
            title_match = re.search(r'<h3[^>]*>\s*<a[^>]*>(.*?)</a>', html, re.DOTALL)
            if title_match:
                title = re.sub(r'<[^>]+>', '', title_match.group(1)).strip()
                if not title or len(title) < 2:
                    title_match2 = re.search(r'title="([^"]+)"', html)
                    if title_match2:
                        title = title_match2.group(1).strip()
            
            # Lien
            link = ""
            link_match = re.search(r'href="(/annonces/[^"]+)"', html)
            if link_match:
                link = self.base_url + link_match.group(1)
            else:
                # Fallback pour lien
                link_match = re.search(r'href="(/offre/[^"]+)"', html)
                if link_match:
                    link = self.base_url + link_match.group(1)
            
            # Entreprise
            company = "Non sp√©cifi√©"
            company_match = re.search(r'/profil-entreprise/([^"/]+)', html)
            if company_match:
                company = company_match.group(1).replace('-', ' ').title()
            else:
                company_match = re.search(r'<span[^>]*class="[^"]*company[^"]*"[^>]*>(.*?)</span>', html, re.DOTALL)
                if company_match:
                    company = re.sub(r'<[^>]+>', '', company_match.group(1)).strip()
            
            # Date
            date_str = "Aujourd'hui"
            date_match = re.search(r'<span[^>]*class="[^"]*date-pub[^"]*"[^>]*>(.*?)</span>', html, re.DOTALL)
            if date_match:
                date_str = re.sub(r'<[^>]+>', '', date_match.group(1)).strip()
            
            # Type de contrat
            contrat = "Non sp√©cifi√©"
            contrat_match = re.search(r'<span[^>]*class="[^"]*contrat-type[^"]*"[^>]*>(.*?)</span>', html, re.DOTALL)
            if contrat_match:
                contrat = contrat_match.group(1).strip()
            
            # Secteur
            secteur = "Non sp√©cifi√©"
            secteur_match = re.search(r'<a[^>]*href="/emploi/s-[^"]*"[^>]*>(.*?)</a>', html, re.DOTALL)
            if secteur_match:
                secteur = secteur_match.group(1).strip()
            
            # M√©tier
            metier = "Non sp√©cifi√©"
            metier_match = re.search(r'<a[^>]*href="/emploi/m-[^"]*"[^>]*>(.*?)</a>', html, re.DOTALL)
            if metier_match:
                metier = metier_match.group(1).strip()
            
            # Localisation
            location = "Antananarivo"  # Par d√©faut
            location_match = re.search(r'<a[^>]*href="/emploi/v-[^"]*"[^>]*>(.*?)</a>', html, re.DOTALL)
            if location_match:
                location = location_match.group(1).strip()
            
            # Calculer le risque IA
            ia_risk_score = self.calculate_ia_risk(title, metier, secteur, contrat)
            ia_risk_level = self.get_risk_level(ia_risk_score)
            
            # Suggestions de reconversion
            suggestions = self.get_reconversion_suggestions(metier, secteur, ia_risk_score)
            
            # Description (simplifi√©e)
            description = self.extract_description(html)
            
            return {
                'title': title[:200],
                'link': link[:500],
                'company': company[:100],
                'date': self.clean_date(date_str),
                'contrat': contrat[:50],
                'secteur': secteur[:100],
                'metier': metier[:100],
                'location': location[:100],
                'description': description[:500],
                'ia_risk_score': ia_risk_score,
                'ia_risk_level': ia_risk_level,
                'suggestions': suggestions,
                'scraped_at': datetime.now()
            }
            
        except Exception as e:
            print(f"‚ö† Erreur parsing offre: {e}")
            return None
    
    def clean_date(self, date_str):
        """Nettoyer et formater la date"""
        if not date_str or date_str == "Non sp√©cifi√©":
            return datetime.now().strftime("%Y-%m-%d")
        
        date_str = date_str.strip().lower()
        
        # G√©rer les formats fran√ßais
        if "aujourd'hui" in date_str or "today" in date_str:
            return datetime.now().strftime("%Y-%m-%d")
        elif "hier" in date_str or "yesterday" in date_str:
            return (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
        elif "il y a" in date_str:
            # Extraire le nombre de jours
            days_match = re.search(r'(\d+)', date_str)
            if days_match:
                days_ago = int(days_match.group(1))
                return (datetime.now() - timedelta(days=days_ago)).strftime("%Y-%m-%d")
        
        return datetime.now().strftime("%Y-%m-%d")
    
    def extract_description(self, html):
        """Extraire la description simplifi√©e"""
        desc_match = re.search(r'<p[^>]*class="[^"]*description[^"]*"[^>]*>(.*?)</p>', html, re.DOTALL)
        if desc_match:
            description = re.sub(r'<[^>]+>', '', desc_match.group(1)).strip()
            return description[:200] + "..." if len(description) > 200 else description
        return "Description non disponible"
    
    def calculate_ia_risk(self, title, metier, secteur, contrat):
        """Calculer un score de risque d'automatisation par l'IA"""
        score = 5.0
        
        # Combiner tout le texte pour l'analyse
        text = f"{title} {metier} {secteur} {contrat}".lower()
        
        # Score bas√© sur le m√©tier (priorit√© haute)
        metier_risks = {
            'chauffeur': 9.0, 'conducteur': 9.0, 'driver': 9.0,
            'livreur': 8.5, 'delivery': 8.5, 'coursier': 8.5,
            'caissier': 8.0, 'cashier': 8.0,
            't√©l√©op√©rateur': 7.5, 'call center': 7.5, 't√©l√©conseiller': 7.5,
            'secr√©taire': 7.0, 'secretary': 7.0, 'assistant': 6.5,
            'op√©rateur': 7.0, 'operator': 7.0,
            'm√©canicien': 6.0, 'mechanic': 6.0,
            'comptable': 5.0, 'accountant': 5.0,
            'enseignant': 2.0, 'teacher': 2.0, 'professeur': 2.0,
            'm√©decin': 1.5, 'doctor': 1.5,
            'infirmier': 2.0, 'nurse': 2.0,
            'd√©veloppeur': 3.0, 'developer': 3.0,
            'manager': 2.5, 'directeur': 2.0, 'chef': 2.5,
            'coordinateur': 2.0, 'coordinator': 2.0,
            'conseiller': 3.0, 'consultant': 3.0,
        }
        
        for job, risk in metier_risks.items():
            if job in text:
                score = risk
                break  # Prendre le premier m√©tier trouv√©
        
        # Ajustements par secteur
        if any(word in text for word in ['transport', 'logistique', 'delivery']):
            score += 1.0
        if any(word in text for word in ['industrie', 'production', 'manufacturing']):
            score += 1.5
        if any(word in text for word in ['commerce', 'retail', 'supermarket']):
            score += 0.5
        if any(word in text for word in ['technologie', 'tech', 'it', 'informatique']):
            score -= 1.0
        if any(word in text for word in ['sant√©', 'health', 'medical']):
            score -= 1.5
        if any(word in text for word in ['√©ducation', 'education', 'formation']):
            score -= 1.0
        
        # Ajustements par mots-cl√©s
        high_risk_words = ['r√©p√©titif', 'routine', 'standard', 'process', 'assembly']
        for word in high_risk_words:
            if word in text:
                score += 0.5
        
        low_risk_words = ['cr√©atif', 'creative', 'design', 'gestion', 'management', 'relation client']
        for word in low_risk_words:
            if word in text:
                score -= 0.5
        
        # Garder dans les limites
        return round(max(1.0, min(10.0, score)), 1)
    
    def get_risk_level(self, score):
        """Convertir score en niveau de risque"""
        if score >= 8.0:
            return "√âlev√©"
        elif score >= 5.0:
            return "Moyen"
        else:
            return "Faible"
    
    def get_reconversion_suggestions(self, metier, secteur, score):
        """G√©n√©rer des suggestions de reconversion"""
        suggestions = []
        
        if score >= 8.0:  # Risque √©lev√©
            suggestions = [
                "Formation en comp√©tences num√©riques (Excel, outils de gestion)",
                "Reconversion vers la logistique ou la coordination",
                "D√©veloppement de comp√©tences en gestion de projet",
                "Apprentissage des outils de relation client (CRM)"
            ]
        elif score >= 5.0:  # Risque moyen
            suggestions = [
                "Renforcement des comp√©tences relationnelles",
                "Apprentissage des outils digitaux de votre secteur",
                "Sp√©cialisation dans un cr√©neau √† forte valeur ajout√©e"
            ]
        else:  # Risque faible
            suggestions = [
                "Continuer √† se former dans votre domaine",
                "D√©velopper une expertise compl√©mentaire",
                "Renforcer vos comp√©tences en leadership"
            ]
        
        return suggestions
    
    def save_to_database(self, offer_data):
        """Sauvegarder une offre en base de donn√©es - version simplifi√©e"""
        if not self.use_database:
            return False
        
        try:
            db = SessionLocal()
            
            # V√©rifier si l'offre existe d√©j√† (juste par lien)
            existing = db.query(JobOffer.id).filter(
                JobOffer.link == offer_data['link']
            ).first()
            
            if not existing:
                # Cr√©er une nouvelle offre
                job_offer = JobOffer(
                    title=offer_data['title'],
                    link=offer_data['link'],
                    company=offer_data['company'],
                    date_posted=offer_data['date'],
                    contract_type=offer_data['contrat'],
                    sector=offer_data['secteur'],
                    job_title=offer_data['metier'],
                    location=offer_data['location'],
                    description=offer_data['description'],
                    ia_risk_score=offer_data['ia_risk_score'],
                    ia_risk_level=offer_data['ia_risk_level'],
                    suggestions=', '.join(offer_data['suggestions']),
                    scraped_at=datetime.now(),
                    is_active=True
                )
                db.add(job_offer)
                db.commit()
                db.close()
                return True
            else:
                db.close()
                return False  # D√©j√† existante
                
        except Exception as e:
            print(f"‚ùå Erreur base de donn√©es: {e}")
            if 'db' in locals():
                db.rollback()
                db.close()
            return False
    
    def scrape_category(self, category, pages=2):
        """Scraper une cat√©gorie - version simplifi√©e"""
        print(f"\n{'='*60}")
        print(f"üì• SCRAPING: {category.upper()}")
        print(f"{'='*60}")
        
        all_offers = []
        saved_count = 0
        
        for page in range(1, pages + 1):
            if page == 1:
                url = f"{self.base_url}/{category}"
            else:
                url = f"{self.base_url}/{category}?page={page}"
            
            print(f"\nüìÑ Page {page}/{pages}: {url}")
            
            html = self.fetch_page(url)
            if not html:
                print("   ‚è≠Ô∏è  Page vide ou erreur, on continue...")
                continue
            
            offers_html = self.extract_offers_html(html)
            
            if not offers_html:
                print("   ‚ö†  Aucune offre d√©tect√©e sur cette page")
                continue
            
            page_saved = 0
            for i, offer_html in enumerate(offers_html, 1):
                offer_data = self.parse_offer(offer_html)
                if offer_data and offer_data['link']:
                    all_offers.append(offer_data)
                    
                    if self.use_database:
                        if self.save_to_database(offer_data):
                            page_saved += 1
                            saved_count += 1
            
            print(f"   ‚úÖ {page_saved} nouvelles offres sauvegard√©es sur cette page")
            
            # Pause entre les pages pour √™tre gentil
            if page < pages:
                time.sleep(1.5)
        
        # Afficher le r√©sum√©
        if all_offers:
            print(f"\nüìä R√âSULTAT {category.upper()}:")
            print(f"   ‚Ä¢ Offres analys√©es: {len(all_offers)}")
            print(f"   ‚Ä¢ Nouvelles offres sauvegard√©es: {saved_count}")
            
            # Statistiques de risque
            risk_counts = {"√âlev√©": 0, "Moyen": 0, "Faible": 0}
            for offer in all_offers:
                level = offer.get('ia_risk_level', 'Inconnu')
                if level in risk_counts:
                    risk_counts[level] += 1
            
            print(f"   ‚Ä¢ Risque √©lev√©: {risk_counts['√âlev√©']}")
            print(f"   ‚Ä¢ Risque moyen: {risk_counts['Moyen']}")
            print(f"   ‚Ä¢ Risque faible: {risk_counts['Faible']}")
            
            # Exemple d'offre √† haut risque
            high_risk = [o for o in all_offers if o.get('ia_risk_level') == '√âlev√©']
            if high_risk:
                print(f"\n   üö® EXEMPLE √Ä HAUT RISQUE:")
                example = high_risk[0]
                print(f"      Titre: {example['title'][:50]}...")
                print(f"      M√©tier: {example['metier']}")
                print(f"      Score IA: {example['ia_risk_score']}/10")
        
        return all_offers
    
    def scrape_all_for_hackathon(self):
        """Scraper toutes les cat√©gories pour le hackathon"""
        print("\n" + "="*60)
        print("üöÄ LANCEMENT DU SCRAPING POUR LE HACKATHON")
        print("Objectif: R√©cup√©rer au moins 50 offres r√©elles")
        print("="*60)
        
        # Cat√©gories et nombre de pages pour chacune
        categories_config = {
            "cdd": 3,      # CDD - souvent beaucoup d'offres
            "emploi": 5,   # Toutes les offres
            "freelance": 2, # Freelance
            "stage": 2,    # Stages
            "cdi": 2       # CDI
        }
        
        total_offers = []
        total_saved = 0
        
        for category, pages in categories_config.items():
            try:
                offers = self.scrape_category(category, pages=pages)
                if offers:
                    total_offers.extend(offers)
                    # Compter combien ont √©t√© sauvegard√©s
                    for offer in offers:
                        if offer.get('link'):
                            total_saved += 1
            except Exception as e:
                print(f"‚ùå Erreur avec {category}: {e}")
                continue
        
        # R√©sum√© final
        print(f"\n{'='*60}")
        print("üéØ R√âSUM√â FINAL DU SCRAPING")
        print(f"{'='*60}")
        print(f"üìä Total offres analys√©es: {len(total_offers)}")
        print(f"üíæ Offres dans MySQL: {total_saved}")
        
        if total_offers:
            # Statistiques globales
            risk_counts = {"√âlev√©": 0, "Moyen": 0, "Faible": 0}
            metiers = {}
            
            for offer in total_offers:
                level = offer.get('ia_risk_level', 'Inconnu')
                if level in risk_counts:
                    risk_counts[level] += 1
                
                metier = offer.get('metier', 'Inconnu')
                metiers[metier] = metiers.get(metier, 0) + 1
            
            print(f"\nüìà DISTRIBUTION DES RISQUES:")
            for level, count in risk_counts.items():
                percentage = (count / len(total_offers) * 100) if total_offers else 0
                print(f"   ‚Ä¢ {level}: {count} offres ({percentage:.1f}%)")
            
            print(f"\nüèÜ TOP 5 M√âTIERS:")
            top_metiers = sorted(metiers.items(), key=lambda x: x[1], reverse=True)[:5]
            for i, (metier, count) in enumerate(top_metiers, 1):
                print(f"   {i}. {metier}: {count} offres")
            
            # Suggestions pour la d√©mo
            high_risk_offers = [o for o in total_offers if o.get('ia_risk_level') == '√âlev√©']
            if high_risk_offers:
                print(f"\nüí° POUR LA D√âMO DU HACKATHON:")
                print(f"   Vous avez {len(high_risk_offers)} offres √† haut risque!")
                print(f"   Exemples parfaits pour montrer l'impact de l'IA")
        
        return total_offers

def main():
    """Fonction principale simplifi√©e"""
    print("\n" + "="*60)
    print("ü§ñ SCRAPER SAFE AI HACKATHON - VERSION OPTIMIS√âE")
    print("Scraping automatique vers MySQL")
    print("="*60)
    
    # Mode automatique - toujours avec MySQL
    use_mysql = True
    
    scraper = AsakoScraper(use_database=use_mysql)
    
    # Lancer le scraping complet
    offers = scraper.scrape_all_for_hackathon()
    
    # Messages finaux
    if offers:
        print(f"\n‚úÖ SCRAPING TERMIN√â AVEC SUCC√àS!")
        print(f"   ‚Üí {len(offers)} offres analys√©es")
        print(f"   ‚Üí Donn√©es disponibles dans MySQL")
        print(f"\nüéØ PROCHAINES √âTAPES:")
        print("1. D√©marrer l'API: python3 run.py")
        print("2. Tester: curl http://localhost:5000/api/health")
        print("3. V√©rifier: curl http://localhost:5000/api/offers")
    else:
        print(f"\n‚ö†  ATTENTION: Peu ou pas de donn√©es r√©cup√©r√©es")
        print("   V√©rifiez votre connexion internet")
        print("   Le site asako.mg peut √™tre temporairement indisponible")

if __name__ == "__main__":
    main()